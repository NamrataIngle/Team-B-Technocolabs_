# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r05y2x3kO23FJ-Dfl4g8H9qlFwY_ae00
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')
#load the data
data=pd.read_csv(r'/content/drive/MyDrive/internships/prosperLoanData.csv')
data.head()

#pip install pivottablejs

'''from pivottablejs import pivot_ui

pivot_ui(data)'''

#Top five records
data.head()

#last five records
data.tail()

# Count of rows and columns
data.shape

# Data Information
data.info()

# Data Discription
data.describe()

#Unique value's count of LoanStatus column
data['LoanStatus'].value_counts()

"""# Data Cleaning & Preprocessing
1. **Dealing with missing values**
"""

data.isnull().sum().any()

plt.figure(figsize=(12,8))
sns.heatmap(data.isnull(), cbar=False, cmap='inferno')
plt.title('Missing values Heapmap')
plt.show()

# categories the data as categorical and numerical
categorical_data = data.select_dtypes('object')
numeric_data = data.select_dtypes('number')

categorical_data.sample(3)

numeric_data.sample(3)

categorical_data.isna().sum()

'''print('Count of Non-null values out of 113937 records:')
print('Group Key:',categorical_data['GroupKey'].count())'''

#Dropping the columns which contains null values more than 30%
threshold = 0.6 * len(data)
col = categorical_data.columns[categorical_data.isnull().sum() > threshold]
print('Columns with more than 30% Null values:',col)

"""Hence, more than 60% records contain null values in this column so we'll drop it, as it'll not provide any major contribution
for further analysis
"""

categorical_data = categorical_data.drop(columns=col)

categorical_data.shape

#Replacing na values with new catogary named "Unknown"
categorical_data.fillna('Unknown', inplace=True)

threshold = 0.8 * len(data)
col = numeric_data.columns[numeric_data.isnull().sum() > threshold]
col

numeric_data = numeric_data.drop(columns=col)

numeric_data.shape

#dealing with numeric columns: replace n/a values with mean
mean = numeric_data.mean()
numeric_data.fillna(mean, inplace=True)

categorical_data.columns

numeric_data.columns

cleaned_data = pd.concat([categorical_data,numeric_data], axis=1)
cleaned_data.isna().sum().any()

plt.figure(figsize=(12,8))
sns.heatmap(cleaned_data.isnull(), cbar=False, cmap='inferno')
plt.title('Missing values Heapmap')
plt.show()

"""Hence, there are no null values in dataset."""

#Check for duplicates
cleaned_data.duplicated().any()

"""Here we go there are no any duplicates

2.**Data Labeling**
"""

cleaned_data['LoanStatus'].value_counts()

#Converting target variable into binary formate
cleaned_data['Status'] = cleaned_data['ClosedDate'].apply(lambda x: 1 if pd.isnull(x) else 0)

cleaned_data['Status'] = cleaned_data['LoanCurrentDaysDelinquent'].apply(lambda x: 1 if x > 180 else 0)

cleaned_data['Status'].value_counts()     # 1 : Defaulted & 0 : Not Defaulted

cleaned_data.drop('LoanStatus', inplace = True, axis = 1)    #Dropped the original one

cleaned_data['EmploymentStatus'].value_counts()

#Creating labels for 'EmploymentStatus' Column
labels = {
    'Employed': 'Employed',
    'Full-time': 'Employed',
    'Self-employed': 'Self-employed',
    'Not available': 'Not Available',
    'Unknown': 'Not Available',
    'Other': 'Other',
    'Part-time': 'Part-time',
    'Not employed': 'Not Employed',
    'Retired': 'Not Employed'
}

cleaned_data['EmploymentCategory'] = cleaned_data['EmploymentStatus'].map(labels)

cleaned_data.drop('EmploymentStatus', inplace = True, axis = 1)        #Drooping original one

cleaned_data['IncomeRange'].value_counts()

income_range_labels = {

    '$25,000-49,999': 'Low Income',
    '$50,000-74,999': 'Moderate Income',
    '$100,000+': 'Very High Income',
    '$75,000-99,999': 'High Income',
    '$1-24,999': 'Very Low Income',
    'Not displayed': 'Not Displayed',
    'Not employed': 'Unemployed',
    '$0': 'No Income'

}

cleaned_data['IncomeRange'] = cleaned_data['IncomeRange'].map(income_range_labels)

cleaned_data['LoanOriginationQuarter'].unique()

quarter_label = {
    'Q4 2005': 'Q4 2005','Q1 2006': 'Q1 2006 - Q2 2006','Q2 2006': 'Q1 2006 - Q2 2006','Q3 2006': 'Q3 2006 - Q4 2006',
    'Q4 2006': 'Q3 2006 - Q4 2006','Q1 2007': 'Q1 2007 - Q2 2007','Q2 2007': 'Q1 2007 - Q2 2007','Q3 2007': 'Q3 2007 - Q4 2007',
    'Q4 2007': 'Q3 2007 - Q4 2007','Q1 2008': 'Q1 2008 - Q2 2008','Q2 2008': 'Q1 2008 - Q2 2008','Q3 2008': 'Q3 2008 - Q4 2008',
    'Q4 2008': 'Q3 2008 - Q4 2008','Q1 2009': 'Q1 2009 - Q2 2009','Q2 2009': 'Q1 2009 - Q2 2009','Q3 2009': 'Q3 2009 - Q4 2009',
    'Q4 2009': 'Q3 2009 - Q4 2009','Q1 2010': 'Q1 2010 - Q2 2010','Q2 2010': 'Q1 2010 - Q2 2010','Q3 2010': 'Q3 2010 - Q4 2010',
    'Q4 2010': 'Q3 2010 - Q4 2010','Q1 2011': 'Q1 2011 - Q2 2011','Q2 2011': 'Q1 2011 - Q2 2011','Q3 2011': 'Q3 2011 - Q4 2011',
    'Q4 2011': 'Q3 2011 - Q4 2011','Q1 2012': 'Q1 2012 - Q2 2012','Q2 2012': 'Q1 2012 - Q2 2012','Q3 2012': 'Q3 2012 - Q4 2012',
    'Q4 2012': 'Q3 2012 - Q4 2012','Q1 2013': 'Q1 2013 - Q2 2013','Q2 2013': 'Q1 2013 - Q2 2013','Q3 2013': 'Q3 2013 - Q4 2013',
    'Q1 2014': 'Q1 2014'

}

cleaned_data['QuarterRange'] = cleaned_data['LoanOriginationQuarter'].map(quarter_label)
cleaned_data.drop('LoanOriginationQuarter', inplace = True, axis = 1)

"""# Data Visualization"""

#Traget variable
values = cleaned_data['Status'].value_counts()

plt.figure(figsize=(10, 6))
plt.pie(values, labels=['Not Defaulted','Defaulted'],  autopct='%1.1f%%', startangle=140)
plt.title('Loan Status Distribution')
plt.legend()
plt.show()

'''#plt.figure(figsize=(12,8))
sns.barplot(x = cleaned_data['label'].value_counts(), y = cleaned_data['label'].value_counts().index)
plt.title('Count according to LoanStatus')
plt.xlabel('Count')
plt.ylabel('Loan Status')
plt.show()'''

"""# Univariate Analysis"""

# Numeric Variables
numerical_columns = cleaned_data.select_dtypes(include=['number'])

numerical_columns = cleaned_data.select_dtypes(include=['number'])

numerical_columns.hist(bins=20, figsize=(20, 20))
plt.title("Histograms of Numerical Columns")
plt.show()

"""Hence, with the help of above plots we can say that the almost data is squwed."""

numerical_columns.plot(kind='box', vert=False, figsize=(16, 12))
plt.title("Box Plots of Numerical Columns")
plt.show()

"""Hence, above boxplots states that there are outliers in dataset"""

#Countplots of categorical columns
categorical_columns = [ 'ProsperRating (Alpha)', 'BorrowerState', 'IncomeRange', 'EmploymentCategory']
for column in categorical_columns:
    plt.figure(figsize=(10, 5))
    sns.countplot(data=cleaned_data, x=column, order=cleaned_data[column].value_counts().index)
    plt.title(f'Count Plot of {column}')
    plt.xticks(rotation=90)
    plt.show()

"""# Bivariate Analysis"""

'''sns.set(style="ticks")
sns.pairplot(numerical_columns, height=2)
plt.show()'''

#Target variable v/s other features
plt.figure(figsize=(20, 20))
col = 5
row = 13
for i, feature in enumerate(numerical_columns):
    plt.subplot(row, col, i + 1)
    plt.scatter(cleaned_data[feature], cleaned_data['Status'], alpha=0.5)
    plt.title(f'Loan Status vs {feature}')
    plt.xlabel(feature)
    plt.ylabel('Loan Status')
    plt.grid(True)

plt.tight_layout()
plt.show()

#Correlation Heatmap
correlation_matrix = cleaned_data.corr()

plt.figure(figsize=(15, 12))
sns.heatmap(correlation_matrix, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

#distribution of each numerical feature with respect to the Loan status

target = 'Status'
for feature in numerical_columns:
    plt.figure(figsize=(8, 6))
    sns.violinplot(x=target, y=feature, data=cleaned_data, palette='Set2')
    plt.title(f'Violin Plot: {feature} by {target}')
    plt.show()

# Bar plots to compare the distribution of LoanStatus by IncomeRange and ProsperRating (Alpha)
plt.figure(figsize=(12, 6))
sns.set(style="whitegrid")
sns.barplot(x='IncomeRange', y='Status', data=cleaned_data, palette='Set3')
plt.title('Bar Plot: LoanStatus by IncomeRange')
plt.xlabel('IncomeRange')
plt.ylabel('LoanStatus Count')
plt.xticks(rotation=45, ha='right')
plt.show()

plt.figure(figsize=(12, 6))
sns.set(style="whitegrid")
sns.barplot(x='ProsperRating (Alpha)', y='Status', data=cleaned_data, palette='Set3')
plt.title('LoanStatus by ProsperRating (Alpha)')
plt.xlabel('ProsperRating (Alpha)')
plt.ylabel('LoanStatus Count')
plt.xticks(rotation=45, ha='right')
plt.show()

"""Dropping the columns which seems to be not contributing in preediction of loan status"""

'''numeric_columns = cleaned_data.select_dtypes(include='number').columns
numeric_data = cleaned_data[numeric_columns]
numeric_data.columns'''

cols_to_drop = ['ListingNumber','LoanNumber','PercentFunded','Investors','LP_CustomerPayments','LP_CustomerPrincipalPayments',
               'LP_InterestandFees', 'LP_ServiceFees','LP_CollectionFees', 'LP_GrossPrincipalLoss', 'LP_NetPrincipalLoss',
               'LP_NonPrincipalRecoverypayments','Recommendations','InvestmentFromFriendsCount', 'InvestmentFromFriendsAmount',
               'MemberKey','LoanOriginationDate','ListingKey','ListingCreationDate']

"""As these columns will not contribute in predicting loan status of customers, so dropping those will be the great choice"""

cleaned_data = cleaned_data.drop(columns=cols_to_drop)

cleaned_data.shape

#cleaned_data['Is']

"""3. **Label Encoding**"""

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

columns = ['EmploymentCategory','BorrowerState','IncomeRange','ProsperRating (Alpha)','QuarterRange','ClosedDate',
           'DateCreditPulled','FirstRecordedCreditLine','LoanKey']
for column in columns:
    cleaned_data[column] = encoder.fit_transform(cleaned_data[column])

cleaned_data.sample(3)

#Frequency-based Encoding
cleaned_data['EncodedOccupation'] = cleaned_data['Occupation'].rank(method='max').astype(int)

cleaned_data.drop('Occupation', axis=1, inplace=True)

column_data_types = cleaned_data.dtypes

categorical_columns = column_data_types[column_data_types == 'object']
print('Categorical_columns:\n\n',categorical_columns)

"""2. **Dealing with outliers**"""

def outliers_col(cleaned_data, threshold=1.5):
    columns_with_outliers = []

    for column in cleaned_data.select_dtypes(include=['number']).columns:
        # IQR
        Q1 = cleaned_data[column].quantile(0.25)
        Q3 = cleaned_data[column].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - threshold * IQR
        upper_bound = Q3 + threshold * IQR
        outliers = cleaned_data[(cleaned_data[column] < lower_bound) | (cleaned_data[column] > upper_bound)]

        if not outliers.empty:
            columns_with_outliers.append(column)

    return columns_with_outliers

outlier_columns = outliers_col(cleaned_data)
print(outlier_columns)

print('Count of Columns with outliers',len(outlier_columns))
print('Total Count:',len(cleaned_data.columns))

'''all_columns = cleaned_data.columns
remaining_cols = [column for column in all_columns if column not in outlier_columns]
len(remaining_cols)'''

from scipy.stats import mstats

for column in outlier_columns:
    winsorized_values = mstats.winsorize(cleaned_data[column], limits=[0.01, 0.01])
    cleaned_data[column] = winsorized_values

#len(winsorized_values)

for column in outlier_columns:
    plt.figure(figsize=(8, 5))
    sns.boxplot(x=cleaned_data[column], color='skyblue')
    plt.title(f'Boxplot of {column} after Winsorizing')
    plt.show()

'''for column in outlier_columns:
    plt.figure(figsize=(8, 5))
    sns.histplot(cleaned_data[column], kde=True, color='green', bins=30)
    plt.title(f'Distribution of {column} After Winsorizing')
    plt.show()'''

'''numeric_columns = cleaned_data.select_dtypes(include=[np.number])

Q1 = numeric_columns.quantile(0.25)
Q3 = numeric_columns.quantile(0.75)
IQR = Q3 - Q1

lower_bounds = Q1 - 1.5 * IQR
upper_bounds = Q3 + 1.5 * IQR

outliers = ((numeric_columns < lower_bounds) | (numeric_columns > upper_bounds))'''

'''outliers_data = cleaned_data.loc[outliers.any(axis=1)]
print('Outliers (IQR): \n')
print(outliers_data)'''

cleaned_data

cleaned_data.columns

#Transformation
transformed_data = np.sqrt(cleaned_data)

transformed_data.describe()

plt.figure(figsize=(12,8))
sns.heatmap(transformed_data.isnull(), cbar=False, cmap='inferno')
plt.title('Missing values Heapmap')
plt.show()

transformed_data.isna().any().sum()

'''mean = transformed_data.mean()
transformed_data.fillna(mean, inplace=True)'''

"""# EDA Questions

Research Question 1 : **What are the most number of borrowers Credit Grade?**
"""

count = data['CreditGrade'].value_counts()

plt.figure(figsize=(12,8))
sns.set(style='darkgrid')
sns.countplot(data=data, x='CreditGrade', order=count.index)
plt.title('Distribution of Credit Grade')
plt.show()

"""Hence, we can see that most of the count comes under unknown category. And then afterwards most of the borrowers lies in CreditGrade **'C' & 'D'**.

Research Question 2 : **Since there are so much low Credit Grade such as C and D , does it lead to a higher amount of deliquency?**
"""

plt.figure(figsize=(12,8))
sns.countplot(data=data, x='LoanStatus')
plt.show()

"""Research Question 3 : **What is the highest number of BorrowerRate?**"""

plt.figure(figsize=(12,8))
sns.set(style='darkgrid')
sns.histplot(data=data, x='BorrowerRate', bins=20, kde=True)
plt.title('Distribution of Borrower Rate')
plt.show()

"""Hence, we can see that highest number of borrower rate lies in between 0.1 to 0.2.

Research Question 4 : **Since the highest number of Borrower Rate is between 0.1 and 0.2, does the highest number of Lender Yield is between 0.1 and 0.2?**
"""

plt.figure(figsize=(12,8))
sns.set(style='darkgrid')
sns.histplot(data=data, x='LenderYield', bins=20, kde=True)
plt.title('Distribution of Lender Yield')
plt.show()

"""**Yes, the highest number of lender yield also lies in beteen 0.1 to 0.2**

Research Question 5 : **Is the Credit Grade really accurate? Does higher Credit Grade leads to higher Monthly Loan Payment? As for Higher Credit Grade we mean from Grade AA to B**
"""

plt.figure(figsize=(18,5))

plt.subplot(1,2,1)
sns.boxplot(data=data, x='CreditGrade', y='MonthlyLoanPayment')
plt.title('Relationship between CreditGrade and MonthlyLoan Payment')

plt.subplot(1,2,2)
sns.scatterplot(data=data, x='CreditGrade', y='MonthlyLoanPayment')
plt.title('Relationship between CreditGrade and MonthlyLoan Payment')
#plt.legend()
plt.show()

"""Research Question 6 : **Here we look at the Completed Loan Status and Defaulted Rate to determine the accuracy of Credit Grade.**"""

cleaned_data['Status'].value_counts()

sns.barplot(x=data['CreditGrade'], y=cleaned_data['Status'])
plt.title('Relationship between CreditGrade and MonthlyLoan Payment')

"""Research Question 7 : **Now we know the Credit Grade is accurate and is a tool that is used by the organization in determining the person’s creditworthiness. Now we need to understand does the ProsperScore, the custom built risk assesment system is being used in determing borrower’s rate?**"""

plt.figure(figsize=(12,8))
sns.scatterplot(data=cleaned_data, x='ProsperScore', y='BorrowerRate')
plt.title('ProsperScore vs BorrowerRate')
plt.show()

#Correlation between prosperscore and borrowerrate
corr = data['ProsperScore'].corr(data['BorrowerRate'])
print('Correlation between ProsperScore and BorrowerRate:',corr)

"""Hence, correlation score is -0.64 it suggests negative correlation means prosper rate increases borrower rate tends to decrease

# Business Insight

**Since the most important assest of a P2P lending Organization is its ability in using its tool to determine a borrower’s creditworthiness as accurate as possible. The organization would be more confident to market its organization as a great investment for investor to invest in hence leading to more borrower and higher market capitilization and boost revenue growth.**

# Feature Engineering

# Correlation Matrix
"""

#Get Correlation of "Status" with other variables:
plt.figure(figsize=(15,8))
cleaned_data.corr()['Status'].sort_values(ascending = False).plot(kind='bar')
plt.title('Correlation of Loan Status vs Other variables')
plt.show()

cleaned_data['Status'].value_counts()

# Applying PCA: Principle Componant Analysis
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

transformed_data.sample(3)

correlation_threshold = 0.15
correlations = transformed_data.corr()['Status'].abs()
columns_to_drop = correlations[correlations < correlation_threshold].index
transformed_data = transformed_data.drop(columns=columns_to_drop)

transformed_data.shape

transformed_data.columns

transformed_data=transformed_data.drop(columns=['ClosedDate','DateCreditPulled'])

X = transformed_data.drop(columns=['Status'])
y = transformed_data['Status']

scaler = StandardScaler()
scaled_data = scaler.fit_transform(X)

"""# PCA"""

from sklearn.decomposition import PCA
pca = PCA()
pca.fit(scaled_data)
principal_components = pca.transform(scaled_data)
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance_ratio = explained_variance_ratio.cumsum()

principal_components = pca.components_

plt.plot(cumulative_variance_ratio)
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Cumulative Explained Variance vs. Number of Components')
plt.show()

threshold = 0.80
n_components = np.argmax(cumulative_variance_ratio >= threshold) + 1
print(f'Number of components: {n_components}')

plt.axhline(y=threshold, color='g', linestyle='--', label=f'{threshold * 100}% Threshold')
plt.legend()
plt.show()

pca = PCA(n_components=n_components)          # n_components
X_reduced = pca.fit_transform(scaled_data)

"""# Model Building: on Data"""

X.shape

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Random Forest classifier
model_1 = RandomForestClassifier(n_estimators=100, random_state=42)
model_1.fit(X_train, y_train)
rf_predictions = model_1.predict(X_test)

print("Classification Report:\n",classification_report(y_test, rf_predictions))

rf_accuracy = accuracy_score(y_test, rf_predictions)
print(f'Accuracy: {rf_accuracy:.4f}')

# Regularized Logistic Regression
model_2 = LogisticRegression(penalty='l2', C=1.0, random_state=42)
model_2.fit(X_train, y_train)
logreg_predictions = model_2.predict(X_test)

print("Classification Report:\n",classification_report(y_test, logreg_predictions))

logreg_accuracy = accuracy_score(y_test, logreg_predictions)
print(f'Logistic Regression Accuracy: {logreg_accuracy:.4f}')

from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(X_train, y_train)
print(f"before undersampling:\n{y_train.value_counts()}")
print(f"\nafter undersampling:\n{pd.Series(y_resampled).value_counts()}")

#Random Forest classifier
model_3 = RandomForestClassifier(n_estimators=100, random_state=42)
model_3.fit(X_resampled, y_resampled)
rf_predictions = model_3.predict(X_test)

rf_accuracy = accuracy_score(y_test, rf_predictions)
print(f'Accuracy: {rf_accuracy:.4f}')

"""# Mutual Info."""

from sklearn.feature_selection import mutual_info_classif

X = transformed_data.drop(columns=['Status'])
y = transformed_data['Status']

X.shape

x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)

n_components = 10
pca = PCA(n_components=n_components)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

mi_scores = mutual_info_classif(X_train_pca, y_train)

print(mi_scores)

from sklearn.feature_selection import SelectKBest, mutual_info_classif

mi_scores = mutual_info_classif(X_train, y_train)
k = 15
selector = SelectKBest(score_func=mutual_info_classif, k=k)
X_train_selected = selector.fit_transform(X_train, y_train)

selected_feature_indices = selector.get_support(indices=True)

print("Selected feature indices:", data.columns[selected_feature_indices])

transformed_data.columns[selected_feature_indices]

"""# Final Model"""

model_4 = RandomForestClassifier(random_state=42)
model_4.fit(X_train_pca, y_train)

y_pred = model_4.predict(X_test_pca)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

model_5 = LogisticRegression(penalty='l2',  random_state=42)
model_5.fit(X_train_pca, y_train)

y_pred = model_5.predict(X_test_pca)
accuracy = accuracy_score(y_test, logreg_predictions)
print(f"Accuracy: {accuracy}")

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(model_1, X_train, y_train, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean Accuracy:", cv_scores.mean())

y_pred = model_1.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Data: {accuracy}")

cv_scores = cross_val_score(model_2, X_train, y_train, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean Accuracy:", cv_scores.mean())

'''y_pred = model_2.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Data: {accuracy}")'''

cv_scores = cross_val_score(model_3, X_resampled, y_resampled, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean Accuracy:", cv_scores.mean())

'''y_pred = model_3.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Data: {accuracy}")'''

cv_scores = cross_val_score(model_4, X_train_pca, y_train, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean Accuracy:", cv_scores.mean())

'''y_pred = model_1.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Data: {accuracy}")'''

cv_scores = cross_val_score(model_5, X_train_pca, y_train, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean Accuracy:", cv_scores.mean())

'''y_pred = model.predict(X_test_pca)'''

'''accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Data: {accuracy}")'''

import pandas as pd
model_accuracy_dict = { 'Model1: RandomForest': 1.00, 'Model2: Regularized LR': 0.99,"Model3: RandomForest": 1.00,
                       'Model4: RandomForest': 0.99,"Model5: Regularized LR": 0.99}
'''
model 1 & 2 on pca data with 16 features
Model 3 is after balancing the data
Model 4 & 5 : With mutual information and PCA on data
'''
df = pd.DataFrame(list(model_accuracy_dict.items()), columns=['Model', 'Accuracy'])
print(df)

data = {
    'Model': ["Model1", "Model2", "Model3", "Model4", "Model5"],
    'Accuracy': [1.00, 0.99, 1.00, 0.99, 0.99],
    'CrossValAccuracy': [1.00, 0.99, 0.99, 0.99, 0.99],  # Mean Accuracy
}

df = pd.DataFrame(data)

best_model_rf = df['Model'][df['CrossValAccuracy'].idxmax()]
best_model_rf

# saving the model
import pickle
pickle_out = open("model1.pkl", mode = "wb")
pickle.dump(model_2, pickle_out)
pickle_out.close()

"""#Model Deployment"""

!pip install Flask
!pip install flask-ngrok
!pip install pyngrok==4.1.1

!ngrok authtoken 2XR07gZxo8YgvXZxJUdXGNR5bE9_3qiSFf9ixJ63iWLX9JRh1

transformed_data.head()

transformed_data.columns

# Upload the templates.zip while execution
from google.colab import files
uploaded = files.upload()

import zipfile

with zipfile.ZipFile('templates.zip', 'r') as zip_ref:
    zip_ref.extractall('templates')

import flask
from flask import Flask, render_template, request
import pickle
import numpy as np
from pyngrok import ngrok  # Import pyngrok

app = Flask(__name__)

model = pickle.load(open('model1.pkl', 'rb'))

@app.route('/', methods=['GET'])
def home():
    return render_template('index.html')

@app.route('/', methods=['POST'])
def predict():
    input_values = [float(x) for x in request.form.values()]
    input_values = input_values
    inp_features = [input_values]
    prediction = model.predict(scaler.fit_transform(inp_features))
    if prediction == 0:
        return render_template('index.html', prediction_text='Not Defaulted')
    else:
        return render_template('index.html', prediction_text='Defaulted')

if __name__ == '__main__':
    public_url = ngrok.connect(port=5000)
    print(' * ngrok tunnel "', public_url, '"')
    app.run()

ngrok.kill()

