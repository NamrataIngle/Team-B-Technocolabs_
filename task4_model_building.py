# -*- coding: utf-8 -*-
"""task4_model_building.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TVzO717sZTeRMpsqwdWZPVY4q5AmRYnZ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv(r'/content/drive/MyDrive/internships/prosperLoanData.csv')
df.head()

df.shape

df.columns

df.info()

df.describe()

df.describe(include='object')

df.isnull().sum()

df["Status"] = df["LoanCurrentDaysDelinquent"].apply(lambda x: 1 if x > 180 else 0)

"""#Data Preprocessing"""

p=4
print("Number of columns with more than 70% null values:",len(df.columns[df.isnull().sum()>df.shape[0]/p]))
print("Columns:")
df.columns[df.isnull().sum()>df.shape[0]/p]
# more than 25% of values of these columns are null(since even if it means 25%, it is nearly 30k null values)

#Dropping above columns
df=df.drop(df.columns[df.isnull().sum()>df.shape[0]/p],axis=1)
df=df.drop(['ListingNumber','ListingKey','LoanKey','MemberKey'],axis=1) #not revevant to our prediction

df.shape

#dropping duplicates in the dataframe
df=df.drop_duplicates(ignore_index=True)

for i in zip(df.columns[df.isnull().sum()!=0],df[df.columns[df.isnull().sum()!=0]].isnull().sum()):
  print(i)

#these are the columns which still have null values

numerics = ['float64', 'int64']
df_num = df.select_dtypes(include=numerics)

df_num.shape

df_num.head()

df_normalized=df_num.copy()
for column in df_normalized.columns:
    df_normalized[column]=(df_normalized[column]-df_normalized[column].min())/(df_normalized[column].max()-df_normalized[column].min())

df_num.columns

corr=df_num.corr()
sns.heatmap(df_num.corr())

corr['Status']

selected_columns = corr[(corr['Status']>= -0.1) & (corr['Status']<= 0.1)].index
len(selected_columns)
print(selected_columns)

df=df.drop(selected_columns,axis=1)
df.head()

df.shape

df_categorical = set(df.columns) - set(df_num.columns)
df_cat=df[df_categorical]
df_cat.head()

df=df.drop(['FirstRecordedCreditLine','ListingCreationDate','DateCreditPulled','LoanOriginationDate'],axis=1)

print(df.shape)
df.head()

df_categorical = set(df.columns) - set(df_num.columns)
df_cat=df[df_categorical]
df_cat.head()

fig=plt.figure(figsize=(10,6))
grid=plt.GridSpec(1,2,hspace=0.5,wspace=0.2)
plt.subplot(grid[0,0])
colors = sns.color_palette('bright',11)
plt.pie(x=df.LoanStatus.value_counts(),colors=colors,labels=df.LoanStatus.value_counts().index,autopct='%1f%%');
plt.subplot(grid[0,1])
plt.bar(df.LoanStatus.value_counts().index,df.LoanStatus.value_counts())
plt.xticks(rotation=90)

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

df['LoanStatus']= label_encoder.fit_transform(df['LoanStatus'])
label_mapping = dict(zip(label_encoder.transform(label_encoder.classes_),label_encoder.classes_))
print(label_mapping)
df['LoanStatus'].unique()

df['IncomeVerifiable'].value_counts()

df['CurrentlyInGroup'].value_counts()

df['IsBorrowerHomeowner'].value_counts()

df['IncomeVerifiable']=df['IncomeVerifiable'].astype(int)
df['CurrentlyInGroup']=df['CurrentlyInGroup'].astype(int)
df['IsBorrowerHomeowner']=df['IsBorrowerHomeowner'].astype(int)

df['EmploymentStatus']= label_encoder.fit_transform(df['EmploymentStatus'])
label_mapping = dict(zip(label_encoder.transform(label_encoder.classes_),label_encoder.classes_))
print(label_mapping)
df['EmploymentStatus'].unique()

df['IncomeRange']= label_encoder.fit_transform(df['IncomeRange'])
label_mapping = dict(zip(label_encoder.transform(label_encoder.classes_),label_encoder.classes_))
print(label_mapping)
df['IncomeRange'].unique()

df['BorrowerState'].value_counts()

df['BorrowerState']= label_encoder.fit_transform(df['BorrowerState'])
label_mapping = dict(zip(label_encoder.transform(label_encoder.classes_),label_encoder.classes_))
print(label_mapping)
df['BorrowerState'].unique()

df[['LoanOriginatedQuarter', 'LoanOriginatedYear']] = df['LoanOriginationQuarter'].str.split(' ', expand=True)
df['LoanOriginatedQuarter'] = df['LoanOriginatedQuarter'].str.extract('(\d+)')
df=df.drop(['LoanOriginationQuarter'],axis=1)

df.shape

df.columns

df.head()

plt.boxplot(df_normalized,notch=True,patch_artist=True,meanline=True);

#outliers handling

df_status=df.groupby(df.LoanStatus)

#plots after diving dataset based on loan values
fig=plt.figure(figsize=(12,12))
grid=plt.GridSpec(6,2,hspace=0.3,wspace=0.2)
for i in range(6):
    for j in range(2):
        plt.subplot(grid[i,j])
        ax=sns.boxplot(data=df_status.get_group(2*i+j))
        ax.set_xticks([])
        ax.set_title(label_mapping[2*i+j])

pd.set_option('display.max_rows', 999)
pd.set_option('display.max_columns', 999)
pd.set_option('display.width', 999)

percentage_df = (df.groupby(['LoanStatus', 'Status']).size() / df.groupby('LoanStatus').size()).reset_index(name='percentage')
percentage_df

df.head()

df.shape

df.isnull().sum()[df.isnull().sum()!=0]

df=df.dropna(axis=0)

df.isnull().sum()[df.isnull().sum()!=0]

df.shape

df['Status'].value_counts()

from sklearn.utils import resample
majority_class = df[df['Status'] == 0]
minority_class = df[df['Status'] == 1]

undersampled_majority = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)

df_u = pd.concat([undersampled_majority, minority_class])

df_u['Status'].value_counts()

x=df.drop(['Status'],axis=1).values
y=df['Status'].values

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

from sklearn.decomposition import PCA

pca = PCA(n_components = 20)

x_train = pca.fit_transform(x_train)
x_test = pca.transform(x_test)

explained_variance = pca.explained_variance_ratio_
print(explained_variance.sum())

from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression(random_state = 0)
classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)

from sklearn.metrics import confusion_matrix,accuracy_score

cm = confusion_matrix(y_test, y_pred)
print(accuracy_score(y_test,y_pred)*100)

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(x_train, y_train)

y_pred_knn=knn.predict(x_test)

from sklearn.metrics import confusion_matrix,accuracy_score

cm = confusion_matrix(y_test, y_pred_knn)
print(accuracy_score(y_test,y_pred_knn)*100)

model_l1 = LogisticRegression(
    penalty='l1',
    C=1.0,
    solver='liblinear',
    max_iter=100,
    class_weight=None
)
model_l1.fit(x_train,y_train)

y_pred_l1=model_l1.predict(x_test)

from sklearn.metrics import confusion_matrix,accuracy_score

cm = confusion_matrix(y_test, y_pred_l1)
print(accuracy_score(y_test,y_pred_l1)*100)

model_l2 = LogisticRegression(
    penalty='l2',
    C=1.0,
    solver='liblinear',
    max_iter=100,
    class_weight=None
)
model_l2.fit(x_train,y_train)

y_pred_l2=model_l2.predict(x_test)

from sklearn.metrics import confusion_matrix,accuracy_score

cm = confusion_matrix(y_test, y_pred_l2)
print(accuracy_score(y_test,y_pred_l2)*100)

from sklearn.ensemble import RandomForestClassifier

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(x_train, y_train)

y_pred_rf = rf_classifier.predict(x_test)

from sklearn.metrics import confusion_matrix,accuracy_score

cm = confusion_matrix(y_test, y_pred_rf)
print(accuracy_score(y_test,y_pred_rf)*100)

sns.heatmap(cm,annot=True)

datf=df.drop(['LoanStatus'],axis=1)

x=datf.drop(['Status'],axis=1).values
y=datf['Status'].values

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

from sklearn.decomposition import PCA

pca = PCA(n_components = 15)

x_train = pca.fit_transform(x_train)
x_test = pca.transform(x_test)

explained_variance = pca.explained_variance_ratio_
print(explained_variance.sum())

from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression(random_state = 0)
classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)

from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(accuracy_score(y_test,y_pred)*100)

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(x_train, y_train)

y_pred_knn = knn.predict(x_test)

from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test, y_pred_knn)
print(accuracy_score(y_test,y_pred_knn)*100)

model_l1 = LogisticRegression(
    penalty='l1',
    C=1.0,
    solver='liblinear',
    max_iter=100,
    class_weight=None
)
model_l1.fit(x_train,y_train)

y_pred_l1 = model_l1.predict(x_test)

from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test, y_pred_l1)
print(accuracy_score(y_test,y_pred_l1)*100)

model_l2 = LogisticRegression(
    penalty='l2',
    C=1.0,
    solver='liblinear',
    max_iter=100,
    class_weight=None
)
model_l2.fit(x_train,y_train)

y_pred_l2 = model_l2.predict(x_test)

from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test, y_pred_l2)
print(accuracy_score(y_test,y_pred_l2)*100)

from sklearn.ensemble import RandomForestClassifier

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(x_train, y_train)

y_pred_rf = rf_classifier.predict(x_test)

from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test, y_pred_rf)
print(accuracy_score(y_test,y_pred_rf)*100)

sns.heatmap(cm,annot=True)

